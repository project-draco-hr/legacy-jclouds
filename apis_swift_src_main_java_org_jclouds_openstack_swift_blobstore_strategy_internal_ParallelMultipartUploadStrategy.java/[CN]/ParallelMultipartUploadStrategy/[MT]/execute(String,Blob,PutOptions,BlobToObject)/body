{
  return Futures.makeListenable(ioWorkerExecutor.submit(new Callable<String>(){
    @Override public String call() throws Exception {
      String key=blob.getMetadata().getName();
      Payload payload=blob.getPayload();
      MultipartUploadSlicingAlgorithm algorithm=new MultipartUploadSlicingAlgorithm();
      algorithm.calculateChunkSize(payload.getContentMetadata().getContentLength());
      int parts=algorithm.getParts();
      long chunkSize=algorithm.getChunkSize();
      long remaining=algorithm.getRemaining();
      if (parts > 0) {
        CommonSwiftClient client=ablobstore.getContext().unwrap(SwiftApiMetadata.CONTEXT_TOKEN).getApi();
        final Map<Integer,ListenableFuture<String>> futureParts=new ConcurrentHashMap<Integer,ListenableFuture<String>>();
        final Map<Integer,Exception> errorMap=Maps.newHashMap();
        AtomicInteger errors=new AtomicInteger(0);
        int maxRetries=Math.max(minRetries,parts * maxPercentRetries / 100);
        int effectiveParts=remaining > 0 ? parts + 1 : parts;
        try {
          logger.debug(String.format("initiated multipart upload of %s to container %s" + " consisting from %s part (possible max. retries: %d)",key,container,effectiveParts,maxRetries));
          ArrayBlockingQueue<Integer> activeParts=new ArrayBlockingQueue<Integer>(parallelDegree);
          Queue<Part> toRetry=new ConcurrentLinkedQueue<Part>();
          SortedMap<Integer,String> etags=new ConcurrentSkipListMap<Integer,String>();
          CountDownLatch latch=new CountDownLatch(effectiveParts);
          int part;
          while ((part=algorithm.getNextPart()) <= parts) {
            Integer partKey=new Integer(part);
            activeParts.put(partKey);
            prepareUploadPart(container,blob,key,partKey,payload,algorithm.getNextChunkOffset(),chunkSize,etags,activeParts,futureParts,errors,maxRetries,errorMap,toRetry,latch,blob2Object);
          }
          if (remaining > 0) {
            Integer partKey=new Integer(part);
            activeParts.put(partKey);
            prepareUploadPart(container,blob,key,partKey,payload,algorithm.getNextChunkOffset(),remaining,etags,activeParts,futureParts,errors,maxRetries,errorMap,toRetry,latch,blob2Object);
          }
          latch.await();
          while (errors.get() <= maxRetries && toRetry.size() > 0) {
            int atOnce=Math.min(Math.min(toRetry.size(),errors.get()),parallelDegree);
            CountDownLatch retryLatch=new CountDownLatch(atOnce);
            for (int i=0; i < atOnce; i++) {
              Part failedPart=toRetry.poll();
              Integer partKey=new Integer(failedPart.getPart());
              activeParts.put(partKey);
              prepareUploadPart(container,blob,key,partKey,payload,failedPart.getOffset(),failedPart.getSize(),etags,activeParts,futureParts,errors,maxRetries,errorMap,toRetry,retryLatch,blob2Object);
            }
            retryLatch.await();
          }
          if (errors.get() > maxRetries) {
            throw new BlobRuntimeException(String.format("Too many failed parts: %s while multipart upload of %s to container %s",errors.get(),key,container));
          }
          String eTag=client.putObjectManifest(container,key);
          logger.debug(String.format("multipart upload of %s to container %s" + " succeffully finished with %s retries",key,container,errors.get()));
          return eTag;
        }
 catch (        Exception ex) {
          RuntimeException rtex=Throwables2.getFirstThrowableOfType(ex,RuntimeException.class);
          if (rtex == null) {
            rtex=new RuntimeException(ex);
          }
          for (          Map.Entry<Integer,ListenableFuture<String>> entry : futureParts.entrySet()) {
            entry.getValue().cancel(false);
          }
          throw rtex;
        }
      }
 else {
        ListenableFuture<String> futureETag=ablobstore.putBlob(container,blob,options);
        return maxTime != null ? futureETag.get(maxTime,TimeUnit.SECONDS) : futureETag.get();
      }
    }
  }
),ioWorkerExecutor);
}
